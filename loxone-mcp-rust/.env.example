# Loxone MCP Rust - Environment Variables Example
# Copy this file to .env and customize the values for your setup

# === Option 1: Direct Loxone Credentials (Local Development) ===
LOXONE_USER="your-username"
LOXONE_PASS="your-secure-password"
LOXONE_HOST="192.168.1.100"

# === Option 2: Infisical (Team/Production) ===
# Uncomment and set your Infisical credentials:
# INFISICAL_PROJECT_ID="65xxxxxxxxxxxxxxxxxxxxxx"
# INFISICAL_CLIENT_ID="6xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
# INFISICAL_CLIENT_SECRET="st.xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
# INFISICAL_ENVIRONMENT="dev"

# === Option 2b: Self-hosted Infisical Instance ===
# For self-hosted Infisical (Docker on localhost):
# INFISICAL_HOST="http://localhost:8080"
# INFISICAL_PROJECT_ID="proj_xxxxxxxxxxxxxxxxxx"
# INFISICAL_CLIENT_ID="st.xxxxxxxxxxxxxxxxxx"
# INFISICAL_CLIENT_SECRET="st.xxxxxxxxxxxxxxxxxx"
# INFISICAL_ENVIRONMENT="dev"

# For Infisical on your own server:
# INFISICAL_HOST="https://infisical.your-company.com"
# INFISICAL_PROJECT_ID="proj_xxxxxxxxxxxxxxxxxx"
# INFISICAL_CLIENT_ID="st.xxxxxxxxxxxxxxxxxx"
# INFISICAL_CLIENT_SECRET="st.xxxxxxxxxxxxxxxxxx"
# INFISICAL_ENVIRONMENT="production"

# === Network Configuration ===
# Mock server configuration (for testing and development)
MOCK_SERVER_HOST=127.0.0.1
MOCK_SERVER_PORT=8080
MOCK_USER="test_user"
MOCK_PASS="test_password_change_me"

# Network discovery settings
DISCOVERY_DNS_SERVER=8.8.8.8
DISCOVERY_DNS_PORT=53
DISCOVERY_UDP_PORTS=[7777,7700,80,8080]
DISCOVERY_BROADCAST_ADDRESS=255.255.255.255

# Connection timeouts (seconds)
CONNECTION_TIMEOUT=30
IDLE_TIMEOUT=300
MAX_LIFETIME=3600

# === Security & API Configuration ===
# HTTP transport API key (required for HTTP mode)
# Generate a secure random API key: openssl rand -base64 32
HTTP_API_KEY="generate_your_own_secure_api_key_here"

# SSL verification (set to false only for development)
VERIFY_SSL=true

# === Logging & Monitoring ===
# Log Level: debug, info, warn, error
RUST_LOG=info

# Enable structured logging
ENABLE_STRUCTURED_LOGGING=true

# Enable request tracing
ENABLE_REQUEST_TRACING=true

# Discovery timeout in seconds
DISCOVERY_TIMEOUT=10

# === LLM Provider Configuration ===
# Configure AI providers for intelligent home automation suggestions

# Ollama Configuration (PRIMARY - Local LLM, recommended)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_DEFAULT_MODEL=llama3.2:latest
OLLAMA_AUTO_DOWNLOAD=true
OLLAMA_TIMEOUT=60
OLLAMA_ENABLED=true

# OpenAI Configuration (FALLBACK - Cloud LLM)
# Uncomment and set your API key to enable OpenAI fallback
# OPENAI_API_KEY=sk-your-openai-api-key-here
# OPENAI_ORGANIZATION=your-org-id  # Optional
# OPENAI_DEFAULT_MODEL=gpt-4o
# OPENAI_TIMEOUT=30

# Anthropic Configuration (FALLBACK - Cloud LLM)  
# Uncomment and set your API key to enable Anthropic fallback
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here
# ANTHROPIC_DEFAULT_MODEL=claude-3-5-sonnet-20241022
# ANTHROPIC_TIMEOUT=30

# === LLM Provider Selection Strategy ===
LLM_ENABLE_FALLBACK=true
LLM_COST_PRIORITY=0.3        # Weight for cost optimization (0.0-1.0)
LLM_SPEED_PRIORITY=0.4       # Weight for speed optimization (0.0-1.0)  
LLM_QUALITY_PRIORITY=0.3     # Weight for quality optimization (0.0-1.0)
LLM_PREFER_LOCAL=true        # Prefer local providers (Ollama) over cloud

# === LLM Health Monitoring ===
LLM_HEALTH_CHECK_INTERVAL=60     # Health check interval in seconds
LLM_HEALTH_CHECK_TIMEOUT=10      # Health check timeout in seconds  
LLM_HEALTH_FAILURE_THRESHOLD=3   # Failed checks before marking unhealthy
LLM_AUTO_DISABLE_UNHEALTHY=false # Don't auto-disable providers

# === Performance Settings ===
# Maximum number of concurrent connections
MAX_CONNECTIONS=10

# Rate limiting (requests per minute)
RATE_LIMIT=100

# Request timeout (seconds)
REQUEST_TIMEOUT=30