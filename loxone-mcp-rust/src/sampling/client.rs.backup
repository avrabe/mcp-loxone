//! MCP Sampling client interface
//!
//! This module provides the interface for servers to send sampling requests
//! to MCP clients following the proper MCP sampling protocol.

use crate::error::{LoxoneError, Result};
use crate::sampling::{SamplingRequest, SamplingResponse};
use async_trait::async_trait;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, warn, info};

/// Trait for MCP sampling clients
#[async_trait]
pub trait SamplingClient: Send + Sync {
    /// Send a sampling request to the MCP client
    async fn create_message(&self, request: SamplingRequest) -> Result<SamplingResponse>;

    /// Check if sampling is supported by the connected client
    fn is_sampling_supported(&self) -> bool;

    /// Get sampling capability information
    fn get_sampling_capabilities(&self) -> SamplingCapabilities;
}

/// Sampling capabilities supported by the client
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
#[derive(Default)]
pub struct SamplingCapabilities {
    pub supported: bool,
    pub max_tokens: Option<u32>,
    pub supported_models: Vec<String>,
    pub supports_images: bool,
    pub supports_audio: bool,
}


/* Commented out until provider module is fixed
/// Real LLM provider-based sampling client
pub struct LLMSamplingClient {
    provider_factory: Arc<LLMProviderFactory>,
    capabilities: SamplingCapabilities,
}
*/

/* Commented out until provider module is fixed
impl LLMSamplingClient {
    /// Create a new LLM sampling client with provider factory
    pub async fn new(provider_factory: Arc<LLMProviderFactory>) -> Result<Self> {
        // Get capabilities from the factory
        let all_capabilities = provider_factory.get_all_capabilities().await;
        let all_models = provider_factory.list_all_models().await?;
        
        // Calculate combined capabilities from all providers
        let max_tokens = all_capabilities.values()
            .map(|cap| cap.max_tokens)
            .max()
            .unwrap_or(4096);
        
        let model_names: Vec<String> = all_models.iter()
            .map(|model| model.name.clone())
            .collect();
        
        let capabilities = SamplingCapabilities {
            supported: !all_capabilities.is_empty(),
            max_tokens: Some(max_tokens),
            supported_models: model_names,
            supports_images: false, // Not implemented yet
            supports_audio: false,  // Not implemented yet
        };
        
        info!("‚úÖ LLM sampling client initialized with {} providers and {} models", 
              all_capabilities.len(), capabilities.supported_models.len());
        
        Ok(Self {
            provider_factory,
            capabilities,
        })
    }
}

#[async_trait]
impl SamplingClient for LLMSamplingClient {
    async fn create_message(&self, request: SamplingRequest) -> Result<SamplingResponse> {
        if !self.capabilities.supported {
            return Err(LoxoneError::ServiceUnavailable(
                "No LLM providers available".to_string(),
            ));
        }

        info!("ü§ñ Processing real LLM sampling request with {} messages", request.messages.len());
        
        // Use the provider factory to execute the sampling request
        let result = self.provider_factory.sample(&request).await?;
        
        info!("‚úÖ LLM sampling completed using {} provider in {:?}", 
              result.provider_used, result.response_time);
        
        if let Some(cost) = result.estimated_cost {
            debug!("üí∞ Estimated cost: ${:.6}", cost);
        }
        
        if result.used_fallback {
            info!("‚ö†Ô∏è Primary provider unavailable, used fallback: {:?}", result.provider_used);
        }
        
        Ok(result.response)
    }

    fn is_sampling_supported(&self) -> bool {
        self.capabilities.supported
    }

    fn get_sampling_capabilities(&self) -> SamplingCapabilities {
        self.capabilities.clone()
    }
}
*/

/// Mock sampling client for testing and fallback
pub struct MockSamplingClient {
    capabilities: SamplingCapabilities,
    fallback_enabled: bool,
}

impl MockSamplingClient {
    /// Create a new mock sampling client
    pub fn new(fallback_enabled: bool) -> Self {
        Self {
            capabilities: SamplingCapabilities {
                supported: fallback_enabled,
                max_tokens: Some(4000),
                supported_models: vec!["claude-3-sonnet".to_string(), "gpt-4".to_string()],
                supports_images: false,
                supports_audio: false,
            },
            fallback_enabled,
        }
    }
}

#[async_trait]
impl SamplingClient for MockSamplingClient {
    async fn create_message(&self, request: SamplingRequest) -> Result<SamplingResponse> {
        if !self.fallback_enabled {
            return Err(LoxoneError::ServiceUnavailable(
                "Sampling not supported by client".to_string(),
            ));
        }

        debug!(
            "Mock sampling request with {} messages",
            request.messages.len()
        );

        // Generate a mock response based on the request
        let user_message = request
            .messages
            .iter()
            .find(|m| m.role == "user")
            .ok_or_else(|| LoxoneError::InvalidInput("No user message found".to_string()))?;

        let response_text = if let Some(text) = &user_message.content.text {
            generate_mock_response(text, request.system_prompt.as_deref())
        } else {
            "I understand your automation request. However, this is a mock response since no actual LLM client is connected. Please connect an MCP client that supports sampling for real AI-powered automation suggestions.".to_string()
        };

        Ok(SamplingResponse {
            model: "mock-model".to_string(),
            stop_reason: "endTurn".to_string(),
            role: "assistant".to_string(),
            content: crate::sampling::SamplingMessageContent::text(response_text),
        })
    }

    fn is_sampling_supported(&self) -> bool {
        self.capabilities.supported
    }

    fn get_sampling_capabilities(&self) -> SamplingCapabilities {
        self.capabilities.clone()
    }
}

/// Generate a mock response for testing
fn generate_mock_response(user_text: &str, system_prompt: Option<&str>) -> String {
    let user_lower = user_text.to_lowercase();

    // Determine the type of request
    if user_lower.contains("cozy") {
        "üè† **Creating a Cozy Atmosphere** (Mock Response)\n\n\
            Based on your request, I recommend:\n\n\
            **Lighting Adjustments:**\n\
            ‚Ä¢ Dim living room lights to 30% warm white\n\
            ‚Ä¢ Set hallway lighting to 20% for ambient glow\n\
            ‚Ä¢ Turn on accent lighting in key areas\n\n\
            **Climate Control:**\n\
            ‚Ä¢ Adjust temperature to 22¬∞C (72¬∞F)\n\
            ‚Ä¢ Ensure good air circulation\n\n\
            **Blinds & Privacy:**\n\
            ‚Ä¢ Close blinds partially for intimacy\n\
            ‚Ä¢ Maintain some natural light if daytime\n\n\
            *Note: This is a mock response. Connect a real MCP client with sampling support for AI-powered suggestions.*".to_string()
    } else if user_lower.contains("event")
        || user_lower.contains("party")
        || user_lower.contains("meeting")
    {
        "üéâ **Event Preparation** (Mock Response)\n\n\
            For your event, I suggest:\n\n\
            **Lighting Setup:**\n\
            ‚Ä¢ Bright, welcoming entrance lighting\n\
            ‚Ä¢ Appropriate room lighting for event type\n\
            ‚Ä¢ Backup lighting zones activated\n\n\
            **Climate Preparation:**\n\
            ‚Ä¢ Pre-cool/heat space for guest comfort\n\
            ‚Ä¢ Adjust for expected occupancy\n\n\
            **System Readiness:**\n\
            ‚Ä¢ Test all critical systems\n\
            ‚Ä¢ Prepare backup controls\n\n\
            *Note: This is a mock response. Connect a real MCP client with sampling support for AI-powered suggestions.*".to_string()
    } else if user_lower.contains("morning") {
        "‚òÄÔ∏è **Morning Routine** (Mock Response)\n\n\
            Good morning! Here's your routine:\n\n\
            **Gradual Wake-up:**\n\
            ‚Ä¢ Slowly increase bedroom lighting to 50%\n\
            ‚Ä¢ Open blinds to let in natural light\n\n\
            **Comfort Setup:**\n\
            ‚Ä¢ Warm up main living areas\n\
            ‚Ä¢ Activate morning temperature profile\n\n\
            **Productivity Zones:**\n\
            ‚Ä¢ Bright lighting in work areas\n\
            ‚Ä¢ Energizing atmosphere settings\n\n\
            *Note: This is a mock response. Connect a real MCP client with sampling support for AI-powered suggestions.*".to_string()
    } else if user_lower.contains("night") || user_lower.contains("bedtime") {
        "üåô **Bedtime Routine** (Mock Response)\n\n\
            Preparing for a good night's sleep:\n\n\
            **Wind-down Lighting:**\n\
            ‚Ä¢ Dim all lights to 10% warm white\n\
            ‚Ä¢ Turn off bright task lighting\n\n\
            **Security & Comfort:**\n\
            ‚Ä¢ Close all blinds for privacy\n\
            ‚Ä¢ Set sleep temperature (20¬∞C)\n\
            ‚Ä¢ Activate night mode for all systems\n\n\
            **Final Preparations:**\n\
            ‚Ä¢ Turn off non-essential devices\n\
            ‚Ä¢ Enable security features\n\n\
            Sweet dreams! üò¥\n\n\
            *Note: This is a mock response. Connect a real MCP client with sampling support for AI-powered suggestions.*".to_string()
    } else {
        format!(
            "üè† **Home Automation Suggestion** (Mock Response)\n\n\
            I understand you'd like help with your home automation. Here are some general recommendations:\n\n\
            **Current Analysis:**\n\
            ‚Ä¢ Review your current system state\n\
            ‚Ä¢ Consider energy efficiency opportunities\n\
            ‚Ä¢ Optimize for comfort and convenience\n\n\
            **Suggested Actions:**\n\
            ‚Ä¢ Adjust lighting based on time and occupancy\n\
            ‚Ä¢ Optimize climate settings for efficiency\n\
            ‚Ä¢ Ensure security features are active\n\n\
            *Note: This is a mock response. For intelligent, context-aware suggestions, connect an MCP client that supports sampling (like Claude Desktop) to get real AI-powered automation recommendations.*\n\n\
            **System Prompt Context:** {}",
            system_prompt.unwrap_or("No system prompt provided")
        )
    }
}

/// Sampling client manager
pub struct SamplingClientManager {
    client: Arc<dyn SamplingClient>,
    capabilities: Arc<RwLock<SamplingCapabilities>>,
}

impl SamplingClientManager {
    /// Create a new sampling client manager with real LLM providers
    /* Commented out until provider module is fixed
    pub async fn new_with_providers(provider_factory: Arc<LLMProviderFactory>) -> Result<Self> {
        let client = Arc::new(LLMSamplingClient::new(provider_factory).await?);
        let capabilities = Arc::new(RwLock::new(client.get_sampling_capabilities()));

        info!("üß† Sampling client manager initialized with real LLM providers");
        Ok(Self {
            client,
            capabilities,
        })
    }
    */

    /// Create a new sampling client manager with mock client
    pub fn new_with_mock(fallback_enabled: bool) -> Self {
        let client = Arc::new(MockSamplingClient::new(fallback_enabled));
        let capabilities = Arc::new(RwLock::new(client.get_sampling_capabilities()));

        debug!("üß™ Sampling client manager initialized with mock client");
        Self {
            client,
            capabilities,
        }
    }

    /// Check if sampling is available
    pub async fn is_available(&self) -> bool {
        self.client.is_sampling_supported()
    }

    /// Send a sampling request
    pub async fn request_sampling(&self, request: SamplingRequest) -> Result<SamplingResponse> {
        if !self.client.is_sampling_supported() {
            warn!("Sampling request attempted but not supported by client");
            return Err(LoxoneError::ServiceUnavailable(
                "Sampling not supported by connected MCP client. Please use a client like Claude Desktop that supports the sampling protocol.".to_string(),
            ));
        }

        debug!(
            "Sending sampling request with {} messages",
            request.messages.len()
        );

        match self.client.create_message(request).await {
            Ok(response) => {
                debug!("Sampling response received from model: {}", response.model);
                Ok(response)
            }
            Err(e) => {
                warn!("Sampling request failed: {}", e);
                Err(e)
            }
        }
    }

    /// Get current capabilities
    pub async fn get_capabilities(&self) -> SamplingCapabilities {
        self.capabilities.read().await.clone()
    }

    /// Update capabilities (called when client capabilities change)
    pub async fn update_capabilities(&self, capabilities: SamplingCapabilities) {
        let mut caps = self.capabilities.write().await;
        *caps = capabilities;
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::sampling::SamplingMessage;

    #[tokio::test]
    async fn test_mock_sampling_client() {
        let client = MockSamplingClient::new(true);
        assert!(client.is_sampling_supported());

        let request = SamplingRequest::new(vec![SamplingMessage::user(
            "Make my home cozy for the evening",
        )]);

        let response = client.create_message(request).await.unwrap();
        assert_eq!(response.role, "assistant");
        assert!(response.content.text.unwrap().contains("cozy"));
    }

    #[tokio::test]
    async fn test_sampling_manager() {
        let manager = SamplingClientManager::new_with_mock(true);
        assert!(manager.is_available().await);

        let request =
            SamplingRequest::new(vec![SamplingMessage::user("Prepare for a movie night")]);

        let response = manager.request_sampling(request).await.unwrap();
        assert!(response.content.text.unwrap().contains("Event Preparation"));
    }
}
